{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.extend import primitive, defvjp\n",
    "from autograd.misc.optimizers import adam\n",
    "from scipy.special import lambertw as lambertw_\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import autograd.scipy.stats.norm as gnorm\n",
    "\n",
    "npr.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambertw = primitive(lambda x: lambertw_(x,0).real)\n",
    "defvjp(lambertw, \n",
    "           lambda ans, x: lambda g: 1e-1 * g * ans/(x*(1+ans)),\n",
    "           None \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skews = np.arange(-0.15, 0.15, 0.01)\n",
    "\n",
    "def memoized_lambertw(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample data\n",
    "N = 1000\n",
    "D = 50\n",
    "K = 10\n",
    "\n",
    "scale = 1.\n",
    "skew = 0.05\n",
    "theta_o = theta = npr.randn(N,K) \n",
    "beta_o = beta = npr.randn(D,K) \n",
    "\n",
    "loc = np.matmul(theta, beta.T)\n",
    "u = npr.randn(N,D)\n",
    "z = u * scale + loc\n",
    "y = u * np.exp(skew * u) * scale + loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_params(params):\n",
    "    return params[0], params[1], params[2]\n",
    "\n",
    "def lambertw_logpdf(y, loc, log_scale, skew):\n",
    "    scale = np.exp(log_scale)\n",
    "    u = (y - loc)/scale\n",
    "    if skew != 0:\n",
    "        u_ = u*skew\n",
    "        W = lambertw(u_)\n",
    "        z = W/skew\n",
    "        jacobian = W/(u_*(1+W))\n",
    "        return gnorm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "    else:\n",
    "        return gnorm.logpdf(u) - log_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta, beta.T)\n",
    "skews = np.arange(-0.2, 0.2, 0.01)\n",
    "pdf = np.array([np.sum((lambertw_logpdf(y, loc, np.log(1.), skew))) for skew in skews])\n",
    "plt.plot(skews, pdf, 'r-', lw=1, alpha=0.6)\n",
    "# skews.shape\n",
    "# pdf.shape\n",
    "skews[np.where(pdf == np.max(pdf))]\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def skew_function(skew, limit = 0.2, scale = 1.):\n",
    "        return 0.15 * np.tanh(scale * skew)\n",
    "    \n",
    "    def unpack_params(params):\n",
    "        theta = np.reshape(params[:(N*K)], [N, K])\n",
    "        beta = np.reshape(params[(N*K):-1], [D, K])\n",
    "        skew = skew_function(params[-1])\n",
    "#         skew = 0\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew):\n",
    "        scale = np.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = W/(u_*(1+W))\n",
    "            return gnorm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return gnorm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = np.matmul(theta, beta.T)\n",
    "        return -np.sum(lambertw_logpdf(loc, np.log(1.), skew))\n",
    "    return objective, lambertw_logpdf, unpack_params, skew_function\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), skew_function(params[-1]), np.sum(params[:-1]), end ='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([0.001])])\n",
    "objective, lambertw_logpdf, unpack_params, skew_function = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=10000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def unpack_params(params):\n",
    "        theta = anp.reshape(params[:(N*K)], [N, K])\n",
    "        beta = anp.reshape(params[(N*K):], [D, K])\n",
    "        return theta, beta\n",
    "        \n",
    "    def lambertw_logpdf(theta, beta):\n",
    "        loc = anp.matmul(theta, beta.T)\n",
    "        return (gnorm.logpdf(y, loc , 1.))\n",
    "    \n",
    "    def objective(params, t):\n",
    "        theta, beta = unpack_params(params)\n",
    "        return -anp.sum(lambertw_logpdf(theta, beta))\n",
    "    return objective, lambertw_logpdf, unpack_params\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), end ='\\r')\n",
    "    \n",
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten()])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(z)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f = unpack_params(final_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_new = np.matmul(theta_f, beta_f.T) + npr.randn(N,D)\n",
    "plt.hist(z_1.flatten(), 100, normed=True, alpha=0.2)\n",
    "plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "loc = np.mean(z_new)\n",
    "scale = np.std(z_new)\n",
    "x = np.linspace(norm.ppf(0.001, loc, scale), norm.ppf(0.999, loc, scale), 100)\n",
    "plt.plot(np.sort(z_new.flatten()), norm.pdf(np.sort(z_new.flatten()), loc, scale), 'r-', lw=1, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten()])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f = unpack_params(final_params) \n",
    "\n",
    "z_new = np.matmul(theta_f, beta_f.T) + npr.randn(N,D)\n",
    "plt.hist(y.flatten(), 100, normed=True, alpha=0.2)\n",
    "plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "loc = np.mean(z_new)\n",
    "scale = np.std(z_new)\n",
    "x = np.linspace(norm.ppf(0.001, loc, scale), norm.ppf(0.999, loc, scale), 100)\n",
    "plt.plot(np.sort(z_new.flatten()), norm.pdf(np.sort(z_new.flatten()), loc, scale), 'r-', lw=1, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    def unpack_params(params):\n",
    "#         theta = anp.reshape(params[:(N*K)], [N, K])\n",
    "#         beta = anp.reshape(params[(N*K):-1], [D, K])\n",
    "        skew = 0.2 * anp.tanh(params[-1])\n",
    "#         skew = params[-1]\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew):\n",
    "        scale = anp.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = W/(u_*(1+W))\n",
    "            return gnorm.logpdf(z) + anp.log(anp.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return gnorm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = anp.matmul(theta, beta.T)\n",
    "        return -anp.sum(lambertw_logpdf(loc, np.exp(1.), skew))\n",
    "    return objective, lambertw_logpdf, unpack_params\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), 0.2*anp.tanh(params[-1]), end ='\\r')\n",
    "   \n",
    "\n",
    "# theta_n = npr.randn(N,K)\n",
    "# beta_n = npr.randn(D,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([-0.1])])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "scale = 1.\n",
    "#y = u * np.exp(skew * u) * scale + loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2 * anp.tanh(skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(-10000,10, step = 1)\n",
    "plt.plot(k, lambertw(k), 'r-', lw=1, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(0.05 * npr.randn(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ab7fe9c2f21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# xs = np.linspace(0.0, 1.0, 300)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmemoized_lambertw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulate_lambertw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmemoized_lambertw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# plt.plot(xs, memoized_lambert(xs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# plt.plot(xs, lambertw(xs).real)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ab7fe9c2f21f>\u001b[0m in \u001b[0;36mlw\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1000"
     ]
    }
   ],
   "source": [
    "from scipy.special import lambertw\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def populate_lambertw(a, b, resolution):\n",
    "    x = np.linspace(a, b, resolution * (b - a) + 1)\n",
    "    y = dict(zip(x, lambertw(x,0).real))\n",
    "    def lw(i):\n",
    "        i = int((i - a) * resolution)\n",
    "        return y[i]\n",
    "    return lw\n",
    "\n",
    "\n",
    "# xs = np.linspace(0.0, 1.0, 300)\n",
    "memoized_lambertw = populate_lambertw(-5, 5, 100)\n",
    "memoized_lambertw(5)\n",
    "# plt.plot(xs, memoized_lambert(xs))\n",
    "# plt.plot(xs, lambertw(xs).real)\n",
    "# plt.show()\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambertw_(-0.38, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
