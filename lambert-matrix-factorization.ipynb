{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.extend import primitive, defvjp\n",
    "from autograd.misc.optimizers import adam\n",
    "import scipy.special.lambertw as lambertw_\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npr.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambertw(x):\n",
    "    if np.any(x < -1/np.e):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return lambertw_(x,0).real\n",
    "    \n",
    "lambertw = primitive(lambertw)\n",
    "\n",
    "defvjp(lambertw, \n",
    "#            lambda ans, x: lambda g: g * ans/(x*(1+ans)),\n",
    "        lambda ans, x: lambda g:  g * 1./ (x + np.exp(ans)),\n",
    "        None \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample data\n",
    "N = 1000\n",
    "D = 50\n",
    "K = 5\n",
    "\n",
    "scale = 1.\n",
    "skew = -0.05\n",
    "theta_o = theta = npr.randn(N,K) \n",
    "beta_o = beta = npr.randn(D,K) * np.array([0.1,0.5,1,2,2.5])\n",
    "\n",
    "loc = np.matmul(theta, beta.T)\n",
    "u = npr.randn(N,D)\n",
    "# z = u * scale + loc\n",
    "y = u * np.exp(skew * u) * scale + loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def skew_function(skew, limit = 0.2, scale = 0.5):\n",
    "        return 0.2 * np.tanh(scale * skew)\n",
    "\n",
    "    def unpack_params(params):\n",
    "        theta = np.reshape(params[:(N*K)], [N, K])\n",
    "        beta = np.reshape(params[(N*K):-1], [D, K])\n",
    "        skew = skew_function(params[-1])\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew, t):\n",
    "        scale = np.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0: #and t > 2000:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = 1./(u_+np.exp(W))\n",
    "            return norm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return norm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = np.matmul(theta, beta.T)\n",
    "        return -np.sum(lambertw_logpdf(loc, np.log(1.), skew, t))\n",
    "    return objective, lambertw_logpdf, unpack_params, skew_function\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), skew_function(params[-1]), np.sum(params[:-1]), end ='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 68086.6130256 -0.0718948934423 -76.3962967269\r"
     ]
    }
   ],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "# theta_n = theta_o\n",
    "# beta_n = beta_o\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([0.0001])])\n",
    "objective, lambertw_logpdf, unpack_params, skew_function = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.01, num_iters=2000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "# Inference ends here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters likelihood:  -68070.3267362\n",
      "Original parameters likelihood:  -70648.2522608\n"
     ]
    }
   ],
   "source": [
    "theta_f, beta_f, skew = unpack_params(final_params)\n",
    "# estimated parameters and likelihood \n",
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "print(\"Estimated parameters likelihood: \", np.sum(lambertw_logpdf(loc, np.log(1.), skew, 0)))\n",
    "# original parameters and likelihood\n",
    "loc = np.matmul(theta_o, beta_o.T)\n",
    "print(\"Original parameters likelihood: \", np.sum(lambertw_logpdf(loc, np.log(1.), -0.05, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta_o, beta_o.T)\n",
    "x1 = (lambertw_logpdf(loc, np.log(1.0), -0.05, 0)).flatten()\n",
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "y1 = (lambertw_logpdf(loc, np.log(1.0), skew, 0)).flatten()\n",
    "\n",
    "\n",
    "plt.plot(x1, y1, 'x')\n",
    "# plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "# z = u * scale + loc\n",
    "skews = np.arange(-0.2, 0.2, 0.01)\n",
    "mse = [np.mean((y - (u * np.exp(s * u) * scale + loc))**2) for s in skews]\n",
    "plt.plot(skews, mse, 'r-', lw=1, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta, beta, skew = unpack_params(final_params)\n",
    "# loc = np.matmul(theta_o, beta_o.T)\n",
    "# -np.sum(lambertw_logpdf(loc, np.log(1.), -0.05, 2000)) \n",
    "# skews = np.arange(-1, 1, 0.01)\n",
    "# pdf = np.copy(skews)\n",
    "\n",
    "theta, beta, skew = unpack_params(final_params)\n",
    "loc = np.matmul(theta, beta.T)\n",
    "\n",
    "for i,s in enumerate(skews):\n",
    "    pdf[i] = np.sum(lambertw_logpdf(loc, np.log(1.0), s, 5000))\n",
    "plt.plot(skews, pdf, 'r-', lw=1, alpha=0.6)\n",
    "# skews[np.where(pdf == np.max(pdf))]\n",
    "\n",
    "theta, beta, skew = unpack_params(final_params)\n",
    "loc = np.matmul(theta, beta.T)\n",
    "np.mean(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta, beta, skew = unpack_params(final_params)\n",
    "# loc = np.matmul(theta, beta.T)\n",
    "\n",
    "objective(params, 0)\n",
    "\n",
    "z_new = (u * np.exp(skew * u))\n",
    "# plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "# plt.hist(z.flatten(), 100, normed=True, alpha=0.2)\n",
    "y_new = u * np.exp(skew * u) * scale + loc\n",
    "# plt.show()\n",
    "print(y_new[1,])\n",
    "y[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_params(params):\n",
    "    return params[0], params[1], params[2]\n",
    "\n",
    "def lambertw_logpdf(y, loc, log_scale, skew):\n",
    "    scale = np.exp(log_scale)\n",
    "    u = (y - loc)/scale\n",
    "    if skew != 0:\n",
    "        u_ = u*skew\n",
    "        W = lambertw(u_)\n",
    "        z = W/skew\n",
    "        jacobian = W/(u_*(1+W))\n",
    "        return norm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "    else:\n",
    "        return norm.logpdf(u) - log_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_o = theta = npr.randn(N,K) \n",
    "# beta_o = beta = npr.randn(D,K) \n",
    "# loc = np.matmul(theta, beta.T)\n",
    "# skews = np.arange(-1, 1, 0.01)\n",
    "# pdf = np.array([np.sum((lambertw_logpdf(y, loc, np.log(1.), skew))) for skew in skews])\n",
    "# plt.plot(skews, pdf, 'r-', lw=1, alpha=0.6)\n",
    "# skews[np.where(pdf == np.max(pdf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def skew_function(skew, limit = 0.2, scale = 1.):\n",
    "#         return 0.15 * np.tanh(scale * skew)\n",
    "        return skew\n",
    "    \n",
    "    def unpack_params(params):\n",
    "        theta = np.reshape(params[:(N*K)], [N, K])\n",
    "        beta = np.reshape(params[(N*K):-1], [D, K])\n",
    "#         skew = skew_function(params[-1])\n",
    "        skew = -0.1\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew):\n",
    "        scale = np.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = W/(u_*(1+W))\n",
    "            return gnorm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return gnorm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = np.matmul(theta, beta.T)\n",
    "        return -np.sum(lambertw_logpdf(loc, np.log(1.), skew))\n",
    "    return objective, lambertw_logpdf, unpack_params, skew_function\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), skew_function(params[-1]), np.sum(params[:-1]), end ='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([-0.1])])\n",
    "objective, lambertw_logpdf, unpack_params, skew_function = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=10000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def unpack_params(params):\n",
    "        theta = anp.reshape(params[:(N*K)], [N, K])\n",
    "        beta = anp.reshape(params[(N*K):], [D, K])\n",
    "        return theta, beta\n",
    "        \n",
    "    def lambertw_logpdf(theta, beta):\n",
    "        loc = anp.matmul(theta, beta.T)\n",
    "        return (gnorm.logpdf(y, loc , 1.))\n",
    "    \n",
    "    def objective(params, t):\n",
    "        theta, beta = unpack_params(params)\n",
    "        return -anp.sum(lambertw_logpdf(theta, beta))\n",
    "    return objective, lambertw_logpdf, unpack_params\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), end ='\\r')\n",
    "    \n",
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten()])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(z)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f = unpack_params(final_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_new = np.matmul(theta_f, beta_f.T) + npr.randn(N,D)\n",
    "plt.hist(z_1.flatten(), 100, normed=True, alpha=0.2)\n",
    "plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "loc = np.mean(z_new)\n",
    "scale = np.std(z_new)\n",
    "x = np.linspace(norm.ppf(0.001, loc, scale), norm.ppf(0.999, loc, scale), 100)\n",
    "plt.plot(np.sort(z_new.flatten()), norm.pdf(np.sort(z_new.flatten()), loc, scale), 'r-', lw=1, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten()])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f = unpack_params(final_params) \n",
    "\n",
    "z_new = np.matmul(theta_f, beta_f.T) + npr.randn(N,D)\n",
    "plt.hist(y.flatten(), 100, normed=True, alpha=0.2)\n",
    "plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "loc = np.mean(z_new)\n",
    "scale = np.std(z_new)\n",
    "x = np.linspace(norm.ppf(0.001, loc, scale), norm.ppf(0.999, loc, scale), 100)\n",
    "plt.plot(np.sort(z_new.flatten()), norm.pdf(np.sort(z_new.flatten()), loc, scale), 'r-', lw=1, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    def unpack_params(params):\n",
    "#         theta = anp.reshape(params[:(N*K)], [N, K])\n",
    "#         beta = anp.reshape(params[(N*K):-1], [D, K])\n",
    "        skew = 0.2 * anp.tanh(params[-1])\n",
    "#         skew = params[-1]\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew):\n",
    "        scale = anp.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = W/(u_*(1+W))\n",
    "            return gnorm.logpdf(z) + anp.log(anp.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return gnorm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = anp.matmul(theta, beta.T)\n",
    "        return -anp.sum(lambertw_logpdf(loc, np.exp(1.), skew))\n",
    "    return objective, lambertw_logpdf, unpack_params\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), 0.2*anp.tanh(params[-1]), end ='\\r')\n",
    "   \n",
    "\n",
    "# theta_n = npr.randn(N,K)\n",
    "# beta_n = npr.randn(D,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([-0.1])])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "scale = 1.\n",
    "#y = u * np.exp(skew * u) * scale + loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2 * anp.tanh(skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(-10000,10, step = 1)\n",
    "plt.plot(k, lambertw(k), 'r-', lw=1, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(0.05 * npr.randn(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import lambertw\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def populate_lambertw(a, b, resolution):\n",
    "    x = np.linspace(a, b, resolution * (b - a) + 1)\n",
    "    y = dict(zip(x, lambertw(x,0).real))\n",
    "    def lw(i):\n",
    "        i = int((i - a) * resolution)\n",
    "        return y[i]\n",
    "    return lw\n",
    "\n",
    "\n",
    "# xs = np.linspace(0.0, 1.0, 300)\n",
    "memoized_lambertw = populate_lambertw(-5, 5, 100)\n",
    "memoized_lambertw(5)\n",
    "# plt.plot(xs, memoized_lambert(xs))\n",
    "# plt.plot(xs, lambertw(xs).real)\n",
    "# plt.show()\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambertw_(-0.38, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
