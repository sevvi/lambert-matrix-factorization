{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.extend import primitive, defvjp\n",
    "from autograd.misc.optimizers import adam\n",
    "import scipy.special.lambertw as lambertw_\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npr.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambertw(x):\n",
    "    if np.any(x < -1/np.e):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return lambertw_(x,0).real\n",
    "    \n",
    "lambertw = primitive(lambertw)\n",
    "\n",
    "defvjp(lambertw, \n",
    "#            lambda ans, x: lambda g: g * ans/(x*(1+ans)),\n",
    "        lambda ans, x: lambda g:  g * 1./ (x + np.exp(ans)),\n",
    "        None \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample data\n",
    "N = 1000\n",
    "D = 100\n",
    "K = 5\n",
    "\n",
    "scale = 1.\n",
    "skew = -0.05\n",
    "theta_o = theta = npr.randn(N,K) \n",
    "beta_o = beta = npr.randn(D,K) #* np.array([0.1,0.5,1,2,2.5])\n",
    "\n",
    "loc = np.matmul(theta, beta.T)\n",
    "u = npr.randn(N,D)\n",
    "# z = u * scale + loc\n",
    "y = u * np.exp(skew * u) * scale + loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def skew_function(skew, limit = 0.2, scale = 0.5):\n",
    "        return 0.2 * np.tanh(scale * skew)\n",
    "#         return skew\n",
    "\n",
    "    def unpack_params(params):\n",
    "        theta = np.reshape(params[:(N*K)], [N, K])\n",
    "        beta = np.reshape(params[(N*K):-1], [D, K])\n",
    "        skew = skew_function(params[-1])\n",
    "#         skew = fskew\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew, t):\n",
    "        scale = np.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0: #and t > 2000:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = 1./(u_+np.exp(W))\n",
    "            return norm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return norm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = np.matmul(theta, beta.T)\n",
    "        return -np.sum(lambertw_logpdf(loc, np.log(1.), skew, t))\n",
    "    return objective, lambertw_logpdf, unpack_params, skew_function\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), skew_function(params[-1]), np.sum(params[:-1]), end ='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 138586.055632 -0.0582607914204 -127.756302768\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-631225d6814d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambertw_logpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskew_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_lambertw_routines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfinal_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtheta_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Inference ends here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(grad, x0, callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m      \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m  \u001b[0;31m# First  moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, i)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n\u001b[1;32m     27\u001b[0m                         \"Try jacobian or elementwise_grad.\")\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0munary_to_nary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/core.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/core.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mingrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_outgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/core.py\u001b[0m in \u001b[0;36madd_outgrads\u001b[0;34m(prev_g_flagged, g)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msparse_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmut_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/core.py\u001b[0m in \u001b[0;36msparse_add\u001b[0;34m(vs, x_prev, x_new)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mx_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_prev\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx_prev\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmut_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/autograd-1.2-py3.6.egg/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36mmut_add\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muntake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmut_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSparseObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "# theta_n = theta_o\n",
    "# beta_n = beta_o\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([0.0001])])\n",
    "objective, lambertw_logpdf, unpack_params, skew_function = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=5000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "# Inference ends here. \n",
    "print(\"\\n\")\n",
    "print(\"Estimated skew:\", skew, \"; Original skew:\", -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters likelihood:  -138523.719418\n",
      "Original parameters likelihood:  -141332.292626\n"
     ]
    }
   ],
   "source": [
    "theta_f, beta_f, skew = unpack_params(final_params)\n",
    "# estimated parameters and likelihood \n",
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "print(\"Estimated parameters likelihood: \", np.sum(lambertw_logpdf(loc, np.log(1.), skew, 0)))\n",
    "# original parameters and likelihood\n",
    "loc = np.matmul(theta_o, beta_o.T)\n",
    "print(\"Original parameters likelihood: \", np.sum(lambertw_logpdf(loc, np.log(1.), -0.05, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta_o, beta_o.T)\n",
    "x1 = (lambertw_logpdf(loc, np.log(1.0), -0.05, 0)).flatten()\n",
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "y1 = (lambertw_logpdf(loc, np.log(1.0), skew, 0)).flatten()\n",
    "\n",
    "\n",
    "plt.plot(x1, y1, 'x')\n",
    "# plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.00000000e-01,  -1.50000000e-01,  -1.00000000e-01,\n",
       "        -5.00000000e-02,  -5.55111512e-17,   5.00000000e-02,\n",
       "         1.00000000e-01,   1.50000000e-01])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113534cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "# z = u * scale + loc\n",
    "skews = np.arange(-0.2, 0.2, 0.01)\n",
    "mse = [np.mean((y - (u * np.exp(s * u) * scale + loc))**2) for s in skews]\n",
    "plt.plot(skews, mse, 'r-', lw=1, alpha=0.6)\n",
    "skews = np.arange(-0.2, 0.2, 0.05)\n",
    "\n",
    "skews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta, beta, skew = unpack_params(final_params)\n",
    "# loc = np.matmul(theta_o, beta_o.T)\n",
    "# -np.sum(lambertw_logpdf(loc, np.log(1.), -0.05, 2000)) \n",
    "# skews = np.arange(-1, 1, 0.01)\n",
    "# pdf = np.copy(skews)\n",
    "\n",
    "theta, beta, skew = unpack_params(final_params)\n",
    "loc = np.matmul(theta, beta.T)\n",
    "\n",
    "for i,s in enumerate(skews):\n",
    "    pdf[i] = np.sum(lambertw_logpdf(loc, np.log(1.0), s, 5000))\n",
    "plt.plot(skews, pdf, 'r-', lw=1, alpha=0.6)\n",
    "# skews[np.where(pdf == np.max(pdf))]\n",
    "\n",
    "theta, beta, skew = unpack_params(final_params)\n",
    "loc = np.matmul(theta, beta.T)\n",
    "np.mean(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta, beta, skew = unpack_params(final_params)\n",
    "# loc = np.matmul(theta, beta.T)\n",
    "\n",
    "objective(params, 0)\n",
    "\n",
    "z_new = (u * np.exp(skew * u))\n",
    "# plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "# plt.hist(z.flatten(), 100, normed=True, alpha=0.2)\n",
    "y_new = u * np.exp(skew * u) * scale + loc\n",
    "# plt.show()\n",
    "print(y_new[1,])\n",
    "y[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_params(params):\n",
    "    return params[0], params[1], params[2]\n",
    "\n",
    "def lambertw_logpdf(y, loc, log_scale, skew):\n",
    "    scale = np.exp(log_scale)\n",
    "    u = (y - loc)/scale\n",
    "    if skew != 0:\n",
    "        u_ = u*skew\n",
    "        W = lambertw(u_)\n",
    "        z = W/skew\n",
    "        jacobian = W/(u_*(1+W))\n",
    "        return norm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "    else:\n",
    "        return norm.logpdf(u) - log_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_o = theta = npr.randn(N,K) \n",
    "# beta_o = beta = npr.randn(D,K) \n",
    "# loc = np.matmul(theta, beta.T)\n",
    "# skews = np.arange(-1, 1, 0.01)\n",
    "# pdf = np.array([np.sum((lambertw_logpdf(y, loc, np.log(1.), skew))) for skew in skews])\n",
    "# plt.plot(skews, pdf, 'r-', lw=1, alpha=0.6)\n",
    "# skews[np.where(pdf == np.max(pdf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def skew_function(skew, limit = 0.2, scale = 1.):\n",
    "#         return 0.15 * np.tanh(scale * skew)\n",
    "        return skew\n",
    "    \n",
    "    def unpack_params(params):\n",
    "        theta = np.reshape(params[:(N*K)], [N, K])\n",
    "        beta = np.reshape(params[(N*K):-1], [D, K])\n",
    "#         skew = skew_function(params[-1])\n",
    "        skew = -0.1\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew):\n",
    "        scale = np.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = W/(u_*(1+W))\n",
    "            return gnorm.logpdf(z) + np.log(np.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return gnorm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = np.matmul(theta, beta.T)\n",
    "        return -np.sum(lambertw_logpdf(loc, np.log(1.), skew))\n",
    "    return objective, lambertw_logpdf, unpack_params, skew_function\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), skew_function(params[-1]), np.sum(params[:-1]), end ='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([-0.1])])\n",
    "objective, lambertw_logpdf, unpack_params, skew_function = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=10000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    \n",
    "    def unpack_params(params):\n",
    "        theta = anp.reshape(params[:(N*K)], [N, K])\n",
    "        beta = anp.reshape(params[(N*K):], [D, K])\n",
    "        return theta, beta\n",
    "        \n",
    "    def lambertw_logpdf(theta, beta):\n",
    "        loc = anp.matmul(theta, beta.T)\n",
    "        return (gnorm.logpdf(y, loc , 1.))\n",
    "    \n",
    "    def objective(params, t):\n",
    "        theta, beta = unpack_params(params)\n",
    "        return -anp.sum(lambertw_logpdf(theta, beta))\n",
    "    return objective, lambertw_logpdf, unpack_params\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), end ='\\r')\n",
    "    \n",
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten()])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(z)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f = unpack_params(final_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_new = np.matmul(theta_f, beta_f.T) + npr.randn(N,D)\n",
    "plt.hist(z_1.flatten(), 100, normed=True, alpha=0.2)\n",
    "plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "loc = np.mean(z_new)\n",
    "scale = np.std(z_new)\n",
    "x = np.linspace(norm.ppf(0.001, loc, scale), norm.ppf(0.999, loc, scale), 100)\n",
    "plt.plot(np.sort(z_new.flatten()), norm.pdf(np.sort(z_new.flatten()), loc, scale), 'r-', lw=1, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_n = npr.randn(N,K)\n",
    "beta_n = npr.randn(D,K)\n",
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten()])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f = unpack_params(final_params) \n",
    "\n",
    "z_new = np.matmul(theta_f, beta_f.T) + npr.randn(N,D)\n",
    "plt.hist(y.flatten(), 100, normed=True, alpha=0.2)\n",
    "plt.hist(z_new.flatten(), 100, normed=True, alpha=0.2)\n",
    "loc = np.mean(z_new)\n",
    "scale = np.std(z_new)\n",
    "x = np.linspace(norm.ppf(0.001, loc, scale), norm.ppf(0.999, loc, scale), 100)\n",
    "plt.plot(np.sort(z_new.flatten()), norm.pdf(np.sort(z_new.flatten()), loc, scale), 'r-', lw=1, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambertw_routines(y):\n",
    "    def unpack_params(params):\n",
    "#         theta = anp.reshape(params[:(N*K)], [N, K])\n",
    "#         beta = anp.reshape(params[(N*K):-1], [D, K])\n",
    "        skew = 0.2 * anp.tanh(params[-1])\n",
    "#         skew = params[-1]\n",
    "        return theta, beta, skew\n",
    "        \n",
    "    def lambertw_logpdf(loc, log_scale, skew):\n",
    "        scale = anp.exp(log_scale)\n",
    "        u = (y - loc)/scale\n",
    "        if skew != 0:\n",
    "            u_ = u*skew\n",
    "            W = lambertw(u_)\n",
    "            z = W/skew\n",
    "            jacobian = W/(u_*(1+W))\n",
    "            return gnorm.logpdf(z) + anp.log(anp.abs(jacobian)) - log_scale\n",
    "        else:\n",
    "            return gnorm.logpdf(u) - log_scale\n",
    "      \n",
    "    def objective(params, t):\n",
    "        theta, beta, skew = unpack_params(params)\n",
    "        loc = anp.matmul(theta, beta.T)\n",
    "        return -anp.sum(lambertw_logpdf(loc, np.exp(1.), skew))\n",
    "    return objective, lambertw_logpdf, unpack_params\n",
    "\n",
    "\n",
    "def callback(params, i, g):\n",
    "    if not i%100: print(i, objective(params, 0), 0.2*anp.tanh(params[-1]), end ='\\r')\n",
    "   \n",
    "\n",
    "# theta_n = npr.randn(N,K)\n",
    "# beta_n = npr.randn(D,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.concatenate([theta_n.flatten(),beta_n.flatten(),np.array([-0.1])])\n",
    "objective, lambertw_logpdf, unpack_params = make_lambertw_routines(y)\n",
    "gradient = grad(objective)\n",
    "final_params = adam(gradient, init_params, step_size=0.1, num_iters=1000, callback = callback)\n",
    "theta_f, beta_f, skew = unpack_params(final_params) \n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.matmul(theta_f, beta_f.T)\n",
    "scale = 1.\n",
    "#y = u * np.exp(skew * u) * scale + loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2 * anp.tanh(skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(-10000,10, step = 1)\n",
    "plt.plot(k, lambertw(k), 'r-', lw=1, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(0.05 * npr.randn(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import lambertw\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def populate_lambertw(a, b, resolution):\n",
    "    x = np.linspace(a, b, resolution * (b - a) + 1)\n",
    "    y = dict(zip(x, lambertw(x,0).real))\n",
    "    def lw(i):\n",
    "        i = int((i - a) * resolution)\n",
    "        return y[i]\n",
    "    return lw\n",
    "\n",
    "\n",
    "# xs = np.linspace(0.0, 1.0, 300)\n",
    "memoized_lambertw = populate_lambertw(-5, 5, 100)\n",
    "memoized_lambertw(5)\n",
    "# plt.plot(xs, memoized_lambert(xs))\n",
    "# plt.plot(xs, lambertw(xs).real)\n",
    "# plt.show()\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambertw_(-0.38, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
